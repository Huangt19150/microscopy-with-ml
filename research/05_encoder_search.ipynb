{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/thuang/Documents/Personal/code/microscopy-with-ml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "WORKING_DIR = \"/Users/thuang/Documents/Personal/code/microscopy-with-ml\"\n",
    "os.chdir(WORKING_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (over encoders) example\n",
    "* **Working example of Augmentation**\n",
    "* optuna is just organizing multiple runs in a wrap. Can simply run the range of my desired tuning in a loop and compare it in mlflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thuang/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from mwm.components.image_processing import get_gt_mask_png, read_image_png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Define Dataset Class\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_list, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_list = image_list # This is when image_list is pre-selected for train/val/test split\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_list[idx])  # Assuming masks have the same name\n",
    "\n",
    "        # Read image and mask\n",
    "        image = read_image_png(img_path)\n",
    "        mask_raw = read_image_png(mask_path)\n",
    "\n",
    "        # Normalize & Convert to tensors\n",
    "        image = image / 255.0  # when import from preprocessed image dir: /norm_images\n",
    "        mask = get_gt_mask_png(mask_raw[:,:,0])[:,:,1:] # leave out the 1st channel (empty), [0 1]\n",
    "\n",
    "        # image = cv2.imread(self.image_paths[idx])\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        # mask = mask.unsqueeze(0).float() / 255.0  # Normalize mask\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# üîπ Define Data Augmentation\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "#########\n",
    "# üîπ Load Image & Mask Paths\n",
    "image_dir = \"artifacts/data_ingestion/norm_images\"\n",
    "mask_dir = \"artifacts/data_ingestion/masks\"\n",
    "training_set_file = \"artifacts/data_ingestion/metadata/training.txt\"\n",
    "image_list = [line.strip() for line in open(training_set_file, \"r\")]\n",
    "\n",
    "# image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir)]\n",
    "# mask_paths = [os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)]\n",
    "\n",
    "dataset = NucleiDataset(image_dir, mask_dir, image_list, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "#########\n",
    "# üîπ Define Training Function\n",
    "def train_model(encoder_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load U-Net with different encoders\n",
    "    model = smp.Unet(encoder_name=encoder_name, encoder_weights=\"imagenet\", classes=2, activation=\"sigmoid\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 3  # For quick testing, increase for better results\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return evaluate_model(model)\n",
    "\n",
    "# üîπ Define Evaluation Function (IoU Score)\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = torch.sigmoid(model(images))  # Convert logits to probabilities\n",
    "            outputs = (outputs > 0.5).float()  # Threshold predictions\n",
    "\n",
    "            intersection = (outputs * masks).sum()\n",
    "            union = (outputs + masks).sum() - intersection\n",
    "            iou = intersection / (union + 1e-6)  # Avoid division by zero\n",
    "            total_iou += iou.item()\n",
    "            count += 1\n",
    "\n",
    "    return total_iou / count  # Average IoU score\n",
    "\n",
    "# üîπ Optimize Encoder Selection Using Optuna\n",
    "def objective(trial):\n",
    "    encoders = [\"resnet34\", \"efficientnet-b0\", \"mobilenet_v2\", \"se_resnext50_32x4d\"]\n",
    "    encoder_name = trial.suggest_categorical(\"encoder_name\", encoders)\n",
    "\n",
    "    iou_score = train_model(encoder_name)\n",
    "    return iou_score  # Higher IoU is better\n",
    "\n",
    "# üîπ Run Hyperparameter Search\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=4)\n",
    "\n",
    "# üîπ Print Best Encoder\n",
    "best_encoder = study.best_params[\"encoder_name\"]\n",
    "print(f\"üèÜ Best Encoder: {best_encoder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare encoders/pretrained weights in a visual/qualitative sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 17:46:51,033: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-03-12 17:46:51,036: INFO: common: json file loaded succesfully from: params.json]\n",
      "[2025-03-12 17:46:51,173: INFO: dataset: Dataset: seg_2ch successfully processed. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:38<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "import mlflow\n",
    "\n",
    "from mwm import logger\n",
    "from mwm.constants import *\n",
    "from mwm.utils.common import read_yaml, load_json\n",
    "from mwm.components.model_architecture import *\n",
    "from mwm.components.dataset import *\n",
    "from mwm.components.image_processing import read_image_png, post_processing_watershed_2ch\n",
    "from mwm.components.metrics import iou_object_labels, measures_at\n",
    "\n",
    "\n",
    "class EvaluationProcessor2Channel:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.thresholds = np.round(np.arange(0.5, 1.0, 0.05), 2)\n",
    "\n",
    "\n",
    "    def prep_evaluation(self, prediction, mask_path):\n",
    "        self.sample_name = os.path.basename(mask_path).split(\".\")[0]\n",
    "\n",
    "        # Convert orginal mask to label\n",
    "        mask_raw = read_image_png(mask_path)\n",
    "        self.labels_gt = measure.label(mask_raw[:,:,0], background=0)\n",
    "\n",
    "        # Convert prediction output to label: \n",
    "        prediction = prediction.permute(1, 2, 0).cpu().numpy()\n",
    "        # TODO: add future denoising step before thresholding\n",
    "        prediction = (prediction > 0.5).astype(np.uint8)\n",
    "        reconstruction = post_processing_watershed_2ch(prediction) # key post-processing logic\n",
    "        self.labels_pred = reconstruction[:mask_raw.shape[0], :mask_raw.shape[1]] # remove padding\n",
    "            \n",
    "\n",
    "    def update_metrics(self):\n",
    "        iou_matrix = iou_object_labels(self.labels_gt, self.labels_pred)\n",
    "        if iou_matrix.size == 0:\n",
    "            mean_object_iou = 0.0\n",
    "        else:\n",
    "            mean_object_iou = np.max(iou_matrix, axis=0).mean()\n",
    "        \n",
    "        # Calculate F1 score at all thresholds\n",
    "        for t in self.thresholds:\n",
    "            f1, precision, recall, jaccard, tp, fp, fn = measures_at(t, iou_matrix)\n",
    "            res = {\n",
    "                \"Sample\": self.sample_name, \n",
    "                \"Threshold\": t, \n",
    "                \"F1\": f1, \n",
    "                \"Precision\": precision, \n",
    "                \"Recall\": recall, \n",
    "                \"Jaccard\": jaccard, \n",
    "                \"MeanObjectIoU\": mean_object_iou,\n",
    "                \"TP\": tp, \n",
    "                \"FP\": fp, \n",
    "                \"FN\": fn\n",
    "                }\n",
    "            self.results.append(res)\n",
    "    \n",
    "\n",
    "    def log_key_metrics_to_mlflow(self):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df_agg = df.drop(columns=[\"Sample\"]).groupby(\"Threshold\").mean().reset_index().sort_values(\"Threshold\", ascending=True)\n",
    "        df_agg_list = df_agg.to_dict(\"records\")\n",
    "        for row_dict in df_agg_list:\n",
    "            metrics = {k: v for k, v in row_dict.items() if k != \"Threshold\"}\n",
    "            mlflow.log_metrics(metrics, step=int(row_dict[\"Threshold\"]*100))\n",
    "        mlflow.log_metric(\"MAF1\", df_agg[\"F1\"].mean())\n",
    "        mlflow.log_metric(\"MAPrecision\", df_agg[\"Precision\"].mean())\n",
    "        mlflow.log_metric(\"MARecall\", df_agg[\"Recall\"].mean())\n",
    "        mlflow.log_metric(\"MAJaccard\", df_agg[\"Jaccard\"].mean())\n",
    "        mlflow.log_param(\"thresholds\", self.thresholds)\n",
    "\n",
    "\n",
    "    def save_results(self, output_path):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "class Evaluator():\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        encoder_weights = \"imagenet\",\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = load_json(params_filepath)\n",
    "\n",
    "        # Make & load model\n",
    "        # self.model_path = self.params.model_file_path\n",
    "        self.encoder = encoder\n",
    "        self.encoder_weights = encoder_weights\n",
    "        \n",
    "        # self.model = make_model(self.params.network)\n",
    "        # self.model.load_state_dict(torch.load(self.model_path))\n",
    "        # logger.info(f\"Model loaded from: {self.model_path}\")\n",
    "        self.model = smp.Unet(self.encoder, encoder_weights=self.encoder_weights, in_channels=3, classes=2, activation=\"sigmoid\")\n",
    "\n",
    "        # Make dataset\n",
    "        self.image_dir = os.path.join(self.config.data_ingestion.unzip_dir, self.config.dataset.image_dir)\n",
    "        self.mask_dir = os.path.join(self.config.data_ingestion.unzip_dir, self.config.dataset.mask_dir)\n",
    "        with open(os.path.join(self.config.data_ingestion.unzip_dir, self.config.dataset.test_set_file), \"r\") as f:\n",
    "            self.image_list_test = f.read().splitlines()\n",
    "        self.test_dataset = make_dataset(self.params.dataset, self.image_dir, self.mask_dir, self.image_list_test)\n",
    "\n",
    "        # Make save path (optional)\n",
    "        if self.params.save_predictions:\n",
    "            model_name = os.path.basename(self.encoder).split(\".\")[0]\n",
    "            self.save_dir = os.path.join(self.config.evaluation.evaluation_dir, f\"{model_name}_{self.encoder_weights}_predictions\")\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    def handle_device(self):\n",
    "        # Move model to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.evaluate_processor = EvaluationProcessor2Channel()\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Evaluate individual sample without batching\n",
    "        batch_progress_bar = tqdm(self.test_dataset, desc=f\"Evaluation\", leave=True)\n",
    "        with torch.no_grad():\n",
    "            for image, _ in batch_progress_bar:\n",
    "                mask_path = self.test_dataset.get_mask_path()\n",
    "                image = image.to(self.device)\n",
    "\n",
    "                # TODO: move to Dataset\n",
    "                # Pad images to match the target size\n",
    "                image = self.pad_images(image)\n",
    "\n",
    "                # TODO: any potnetial issue with not using data loader?\n",
    "                image = image.to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "                # Get prediction\n",
    "                output = self.model(image).squeeze()\n",
    "            \n",
    "                # Evaluate\n",
    "                self.evaluate_processor.prep_evaluation(output, mask_path)\n",
    "                self.evaluate_processor.update_metrics()\n",
    "\n",
    "                if self.params.save_predictions:\n",
    "                    save_path = os.path.join(self.save_dir, os.path.basename(mask_path))\n",
    "\n",
    "                    mask_pred = output.permute(1, 2, 0).cpu().numpy()\n",
    "                    mask_pred_uint8 = (mask_pred > 0.5).astype(np.uint8)\n",
    "                    empty_channel = np.zeros_like(mask_pred_uint8[:,:,0])\n",
    "                    mask_pred_uint8 = np.stack([mask_pred_uint8[:,:,1], empty_channel, mask_pred_uint8[:,:,0]], axis=-1) * 255 # cv2 uses BGR\n",
    "                    cv2.imwrite(save_path, mask_pred_uint8)\n",
    "\n",
    "        mlflow.set_experiment(\"Encoder/Architecture Search\")\n",
    "        with mlflow.start_run():\n",
    "            mlflow.set_tag(\"mlflow.runName\", f\"{self.encoder}_{self.encoder_weights}\")\n",
    "\n",
    "            self.evaluate_processor.log_key_metrics_to_mlflow()\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            save_path = os.path.join(\n",
    "                self.config.evaluation.evaluation_dir,\n",
    "                f\"evaluation_{timestamp}_on_{os.path.basename(self.encoder).split('.')[0]}.csv\"\n",
    "            )\n",
    "            self.evaluate_processor.save_results(save_path)\n",
    "\n",
    "            mlflow.log_param(\"evaluation_save_path\", save_path)\n",
    "            mlflow.log_param(\"encoder\", self.encoder)\n",
    "            mlflow.log_param(\"encoder_weights\", self.encoder_weights)\n",
    "            if self.save_dir:\n",
    "                mlflow.log_param(\"save_predictions_dir\", self.save_dir)\n",
    "                \n",
    "\n",
    "    # TODO: do this in Dataset: use crop and set image_size as a param\n",
    "    @staticmethod\n",
    "    def pad_images(images, target_height=544, target_width=704):\n",
    "        \"\"\"\n",
    "        (Move to Dataset class and consider more flexible resizing options: crop, etc.)\n",
    "        \"\"\"\n",
    "        import torch.nn.functional as F\n",
    "        height, width = images.shape[-2], images.shape[-1]\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "        padding = (0, pad_width, 0, pad_height, 0, 0)  # (left, right, top, bottom)\n",
    "        return F.pad(images, padding, mode='constant', value=0)\n",
    "    \n",
    "#########\n",
    "# Main\n",
    "# Log:\n",
    "# 01: resnet34, imagenet\n",
    "# 02: efficientnet-b0, imagenet: ‚àö\n",
    "# 03: efficientnet-b1, imagenet: (inversed)\n",
    "# 04: mobilenet_v2, imagenet\n",
    "# 05: se_resnext50_32x4d, imagenet: Connection refused -- TODO: download manually\n",
    "# 06: resnet50, imagenet\n",
    "# 07: resnet50, ssl\n",
    "# 08: resnet50, swsl\n",
    "# 09: desenet121, imagenet: Connection refused -- TODO: download manually\n",
    "# 10: efficientnet-b1, advprop\n",
    "#  - switch channel order : first channel (R) as full-foreground\n",
    "# 11: efficientnet-b0, advprop: ‚àö‚àö\n",
    "# 12: efficientnet-b2, advprop: ? (features definitely recognized but inversed - 0s in object and 1s in bkg)\n",
    "# 13: efficientnet-b3, advprop: ‚àö‚àö (makes good visual sense! MeanObjectIoU is low but doesn't matter as metric is not sensitive at this low end)\n",
    "# 14: efficientnet-b2, imagenet: ‚àö‚àö (visually the closest! (density and location in both channnel). Simply noisy!)\n",
    "# 15: efficientnet-b3, imagenet\n",
    "\n",
    "evaluator = Evaluator(\"efficientnet-b3\")\n",
    "evaluator.handle_device()\n",
    "evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConvNeCt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_atto',\n",
       " 'convnext_atto_ols',\n",
       " 'convnext_atto_rms',\n",
       " 'convnext_base',\n",
       " 'convnext_femto',\n",
       " 'convnext_femto_ols',\n",
       " 'convnext_large',\n",
       " 'convnext_large_mlp',\n",
       " 'convnext_nano',\n",
       " 'convnext_nano_ols',\n",
       " 'convnext_pico',\n",
       " 'convnext_pico_ols',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'convnext_tiny_hnf',\n",
       " 'convnext_xlarge',\n",
       " 'convnext_xxlarge',\n",
       " 'convnext_zepto_rms',\n",
       " 'convnext_zepto_rms_ols',\n",
       " 'convnextv2_atto',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_femto',\n",
       " 'convnextv2_huge',\n",
       " 'convnextv2_large',\n",
       " 'convnextv2_nano',\n",
       " 'convnextv2_pico',\n",
       " 'convnextv2_small',\n",
       " 'convnextv2_tiny',\n",
       " 'test_convnext',\n",
       " 'test_convnext2',\n",
       " 'test_convnext3']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model_list = [m for m in timm.list_models() if \"convnext\" in m]\n",
    "model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 18:28:07,236: INFO: _builder: Loading pretrained weights from Hugging Face hub (timm/convnext_base.fb_in22k_ft_in1k)]\n",
      "[2025-03-10 18:28:07,873: INFO: _hub: [timm/convnext_base.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.]\n",
      "[2025-03-10 18:28:08,329: INFO: _builder: Missing keys (head.fc.weight, head.fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.]\n",
      "<timm.models._features.FeatureInfo object at 0x336c10490>\n"
     ]
    }
   ],
   "source": [
    "# Select model\n",
    "encoder_name = \"convnext_base\"  # Change if needed\n",
    "\n",
    "# Load model with feature extraction\n",
    "model = timm.create_model(encoder_name, pretrained=True, features_only=True)\n",
    "\n",
    "# Print available feature maps\n",
    "print(model.feature_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 17:12:30,253: INFO: _builder: Loading pretrained weights from Hugging Face hub (timm/convnext_base.fb_in22k_ft_in1k)]\n",
      "[2025-03-10 17:17:41,717: INFO: _hub: [timm/convnext_base.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.]\n",
      "[2025-03-10 17:17:42,341: INFO: _builder: Missing keys (head.fc.weight, head.fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Example Usage\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 48\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mConvNeXt_UNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconvnext_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)  \u001b[38;5;66;03m# Example input image\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     y \u001b[38;5;241m=\u001b[39m model(x)\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36mConvNeXt_UNet.__init__\u001b[0;34m(self, encoder_name, num_classes, pretrained)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28msuper\u001b[39m(ConvNeXt_UNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load ConvNeXt as the encoder\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mfeature_info\u001b[38;5;241m.\u001b[39mchannels()  \u001b[38;5;66;03m# Get feature map sizes\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_factory.py:126\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 126\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    135\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/convnext.py:1114\u001b[0m, in \u001b[0;36mconvnext_base\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvnext_base\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConvNeXt:\n\u001b[1;32m   1113\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m3\u001b[39m], dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m])\n\u001b[0;32m-> 1114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_convnext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconvnext_base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/convnext.py:573\u001b[0m, in \u001b[0;36m_create_convnext\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_cfg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfcmae\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# NOTE fcmae pretrained weights have no classifier or final norm-layer (`head.norm`)\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# This is workaround loading with num_classes=0 w/o removing norm-layer.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_strict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConvNeXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_sequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_builder.py:478\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, cache_dir, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_fmt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_getter:  \u001b[38;5;66;03m# don't set default for intermediate feat getter\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     feature_cfg\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_fmt\u001b[39m\u001b[38;5;124m'\u001b[39m, output_fmt)\n\u001b[0;32m--> 478\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeature_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m model\u001b[38;5;241m.\u001b[39mpretrained_cfg \u001b[38;5;241m=\u001b[39m pretrained_cfg_for_features(pretrained_cfg)  \u001b[38;5;66;03m# add back pretrained cfg\u001b[39;00m\n\u001b[1;32m    480\u001b[0m model\u001b[38;5;241m.\u001b[39mdefault_cfg \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpretrained_cfg  \u001b[38;5;66;03m# alias for rename backwards compat (default_cfg -> pretrained_cfg)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:336\u001b[0m, in \u001b[0;36mFeatureListNet.__init__\u001b[0;34m(self, model, out_indices, output_fmt, feature_concat, flatten_sequential)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    322\u001b[0m         model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m         flatten_sequential: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        model: Model from which to extract features.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m        flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_fmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_concat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflatten_sequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflatten_sequential\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:270\u001b[0m, in \u001b[0;36mFeatureDictNet.__init__\u001b[0;34m(self, model, out_indices, out_map, output_fmt, feature_concat, flatten_sequential)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 270\u001b[0m return_layers \u001b[38;5;241m=\u001b[39m \u001b[43m_get_return_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m modules \u001b[38;5;241m=\u001b[39m _module_list(model, flatten_sequential\u001b[38;5;241m=\u001b[39mflatten_sequential)\n\u001b[1;32m    272\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(return_layers\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:223\u001b[0m, in \u001b[0;36m_get_return_layers\u001b[0;34m(feature_info, out_map)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_return_layers\u001b[39m(feature_info, out_map):\n\u001b[0;32m--> 223\u001b[0m     module_names \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     return_layers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(module_names):\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:141\u001b[0m, in \u001b[0;36mFeatureInfo.module_name\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodule_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: Optional[Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" feature module name accessor\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodule\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:109\u001b[0m, in \u001b[0;36mFeatureInfo.get\u001b[0;34m(self, key, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Get value by key at specified index (indices)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mif idx == None, returns value for key at each output index\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mif idx is an integer, return value for that feature module index (ignoring output indices)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mif idx is a list/tuple, return value for each module index (ignoring output indices)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[i][key] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]\n",
      "File \u001b[0;32m~/Documents/Personal/code/microscopy-with-ml/venv/lib/python3.11/site-packages/timm/models/_features.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Get value by key at specified index (indices)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mif idx == None, returns value for key at each output index\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03mif idx is an integer, return value for that feature module index (ignoring output indices)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mif idx is a list/tuple, return value for each module index (ignoring output indices)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[key] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_indices]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[i][key] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class ConvNeXt_UNet(nn.Module):\n",
    "    def __init__(self, encoder_name=\"convnext_base\", num_classes=1, pretrained=True):\n",
    "        super(ConvNeXt_UNet, self).__init__()\n",
    "\n",
    "        # Load ConvNeXt as the encoder\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=pretrained, features_only=True, out_indices=(0, 1, 2, 3, 4))\n",
    "        encoder_channels = self.encoder.feature_info.channels()  # Get feature map sizes\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.ModuleList([\n",
    "            self.up_block(encoder_channels[4], encoder_channels[3]),\n",
    "            self.up_block(encoder_channels[3], encoder_channels[2]),\n",
    "            self.up_block(encoder_channels[2], encoder_channels[1]),\n",
    "            self.up_block(encoder_channels[1], encoder_channels[0]),\n",
    "            self.up_block(encoder_channels[0], num_classes, final_layer=True)\n",
    "        ])\n",
    "\n",
    "    def up_block(self, in_channels, out_channels, final_layer=False):\n",
    "        \"\"\"Creates an upsampling block with transposed convolution\"\"\"\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if final_layer:\n",
    "            layers.append(nn.Sigmoid())  # For binary segmentation; change for multi-class\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass\n",
    "        enc_features = self.encoder(x)  # List of feature maps at different levels\n",
    "\n",
    "        # Decoder forward pass\n",
    "        x = enc_features[-1]  # Start from deepest feature map\n",
    "        for i, up in enumerate(self.decoder):\n",
    "            x = up(x)\n",
    "            if i < len(enc_features) - 1:\n",
    "                x = torch.cat([x, enc_features[len(enc_features) - 2 - i]], dim=1)  # Skip connection\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = ConvNeXt_UNet(encoder_name=\"convnext_base\", num_classes=1, pretrained=True)\n",
    "    x = torch.randn(1, 3, 256, 256)  # Example input image\n",
    "    y = model(x)\n",
    "    print(y.shape)  # Should be [1, 1, 256, 256] (for binary segmentation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
